{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Active Task Elicitation\n",
    "\n",
    "- This notebook explores and provides a demonstration of the research paper [Eliciting Human Preferences with Language Models](https://arxiv.org/abs/2310.11589).\n",
    "- Defining a desired and unambigous specification for a target task can be challenging.\n",
    "- Learning frameworks may take different approaches in defining the task specification.\n",
    "    - Passive: The user provides a single initial specification.\n",
    "    - Interactive: The specification is updated through multiple queries that can change depending on user responses.\n",
    "    - Example-based: The specification is defined by a set of examples labeled by the user.\n",
    "    - Free-form: The specification is defined by natural language instructions and explanations using language models (LMs).\n",
    "- Existing task elicitation frameworks suffer from passive and example-based approaches.\n",
    "    - Supervised learning: passive, example-based\n",
    "        - The model is provided with labeled examples and then fit and fine-tuned using standard algorithms.\n",
    "        - The effectiveness of the model is highly dependent on the quality of examples and can't be adapted to user preference changes or edge-cases.\n",
    "    - Pool-based active learning: interactive, example-based\n",
    "        - A fixed pool of unlabeled inputs are drawn from interactively for the user to label. The model is then trained as in supervised methods.\n",
    "        - The interactive process enables examples to be drawn that may resolve uncertainty or ambiguity in the task specification.\n",
    "    - Prompting: passive, free-form\n",
    "        - A pre-trained language model is provided with a prompt, which is a natural language description of the task.\n",
    "        - The free-form approach allows for specifying tasks in more flexible ways than simply labeling examples.\n",
    "- All of these existing frameworks have important drawbacks.\n",
    "    - The user must ensure the prompt or example sets are truly comprehensive specifications of the task.\n",
    "    - A poorly crafted prompt or set of examples could lead to task ambiguity resulting in undesired behavior.\n",
    "    - Resolving task ambiguity is challenging and time-consuming due to the difficulties of precisely identifying personal preferences and anticipating edge-cases.\n",
    "- The research paper introduces a learning framework they call generative active task elicitation (GATE).\n",
    "- GATE attempts to solve the challenges of existing frameworks by using an interactive and free-form approach. \n",
    "    - Generative active task elicitation: interactive, free-form\n",
    "        - The model discovers and defines the user's intended task through an open-ended interaction.\n",
    "        - GATE may employ different types of information gathering policies depending on the kind of questions asked.\n",
    "            - Generative active learning\n",
    "                - The LM generates example inputs for the user to label.\n",
    "                - This approach has the advantage of providing scenarios that may not have otherwise been considered.\n",
    "            - Generating yes-or-no questions\n",
    "                - The LM is restricted to generating binary yes-or-no questions.\n",
    "                - This approach enables the model to elicit more abstract preferences while still being easy for the user to answer.\n",
    "            - Generating open-ended questions\n",
    "                - The LM generates arbitrary questions requiring free-form natural language responses.\n",
    "                - This approach is the most flexible at the cost of being overly broad or challenging for the user to answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a demonstration of how GATE works.\n",
    "# For a full implementation to test GATE visit https://github.com/alextamkin/generative-elicitation\n",
    "\n",
    "import os\n",
    "import textwrap\n",
    "from enum import StrEnum\n",
    "from openai import OpenAI\n",
    "\n",
    "# LM configuration\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "engine =  \"gpt-3.5-turbo\"\n",
    "\n",
    "class QuestionType(StrEnum):\n",
    "    YN = \"yes/no question\"\n",
    "    OPEN = \"open-ended question\"\n",
    "\n",
    "# GATE interaction_prompt template.  The result of each query is added to the interaction history.\n",
    "interaction_prompt = textwrap.dedent(\"\"\"\\\n",
    "    Your task is to {task_description}.\n",
    "\n",
    "    Previous questions:\n",
    "    {interaction_history}\n",
    "    Generate the most informative {question_type} that, when answered, will reveal the most about the desired behavior beyond what has already been queried for above.\n",
    "    Make sure your question addresses different aspects of the {implementation} than the questions that have already been asked.\n",
    "    At the same time however, the question should be bite-sized, and not ask for too much at once.\n",
    "    Generate the {question_type} and nothing else:\"\"\"\n",
    ")\n",
    "\n",
    "# Prompt template for testing GATE.\n",
    "hypothesis_prompt = textwrap.dedent(\"\"\"\\\n",
    "    {prompt1}\n",
    "    {previous_examples}\n",
    "    {prompt2}\n",
    "    {test_case}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# GATE updates the prompt over multiple query iterations.\n",
    "def gate(num_interactions, task_description, question_type, implementation):\n",
    "    kwargs = {\n",
    "        \"task_description\": task_description,\n",
    "        \"interaction_history\": \"\",\n",
    "        \"question_type\": question_type,\n",
    "        \"implementation\": implementation\n",
    "    }\n",
    "    for i in range(num_interactions):\n",
    "        messages = [{\"role\": \"user\", \"content\": interaction_prompt.format(**kwargs)}]\n",
    "        completion = client.chat.completions.create(model=engine, messages=messages)\n",
    "        question = completion.choices[0].message.content\n",
    "        answer = input(f\"{question} \")\n",
    "        kwargs[\"interaction_history\"] += f\"- {question} -> {answer}\\n\"\n",
    "    print(f\"LAST INTERACTION PROMPT GENERATED:\\n\\n{interaction_prompt.format(**kwargs)}\")\n",
    "    return kwargs[\"interaction_history\"]\n",
    "\n",
    "def gate_test(prompt1, previous_examples, prompt2, test_cases):\n",
    "    print(\"\\nTEST CASES:\")\n",
    "    kwargs = {\n",
    "        \"prompt1\": prompt1,\n",
    "        \"previous_examples\": previous_examples,\n",
    "        \"prompt2\": prompt2,\n",
    "    }\n",
    "    for test_case, actual in test_cases:\n",
    "        kwargs[\"test_case\"] = test_case\n",
    "        messages = [{\"role\": \"user\", \"content\": hypothesis_prompt.format(**kwargs)}]\n",
    "        completion = client.chat.completions.create(model=engine, messages=messages)\n",
    "        predict = completion.choices[0].message.content\n",
    "        print(f\"\\n{test_case}\\nACTUAL: {actual}, PREDICT: {predict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAST INTERACTION PROMPT GENERATED:\n",
      "\n",
      "Your task is to learn what topics a user is interested in reading online article about.\n",
      "People's interests are broad, so you should seek to understand their interests across many topics; in other words, go for breadth rather than depth.\n",
      "Do not assume a user has given a complete answer to any question, so make sure to keep probing different types of interests.\n",
      "\n",
      "Previous questions:\n",
      "- What types of hobbies or recreational activities do you enjoy? -> I play tennis and I read a lot about it.\n",
      "- What other sports or physical activities do you enjoy participating in or reading about? -> I can’t think of any.\n",
      "- What are some other hobbies or interests that you enjoy exploring online? -> I like to read about science, but  not finance.\n",
      "- What are some other topics or subjects that you enjoy reading about online? -> I have a Scientific American  subscription. I also read the NYT.\n",
      "\n",
      "Generate the most informative open-ended question that, when answered, will reveal the most about the desired behavior beyond what has already been queried for above.\n",
      "Make sure your question addresses different aspects of the user's interests than the questions that have already been asked.\n",
      "At the same time however, the question should be bite-sized, and not ask for too much at once.\n",
      "Generate the open-ended question and nothing else:\n",
      "\n",
      "TEST CASES:\n",
      "\n",
      "Website Name: New York Times\n",
      "Title: What Does the Future Hold for AI?\n",
      "ACTUAL: Yes, PREDICT: Yes\n",
      "\n",
      "Website Name: CBS Sports\n",
      "Title: Top Basketball Trends to Watch.\n",
      "ACTUAL: No, PREDICT: No\n",
      "\n",
      "Website Name: ESPN\n",
      "Title: Behind Serena’s Killer Serve.\n",
      "ACTUAL: Yes, PREDICT: Yes\n"
     ]
    }
   ],
   "source": [
    "# This is an example. You can modify these variables to experiment with different GATE use cases.\n",
    "\n",
    "num_interactions = 4\n",
    "\n",
    "# Variables for the GATE interaction_prompt template.\n",
    "task_description = textwrap.dedent(\"\"\"\\\n",
    "    learn what topics a user is interested in reading online article about.\n",
    "    People's interests are broad, so you should seek to understand their interests across many topics; in other words, go for breadth rather than depth.\n",
    "    Do not assume a user has given a complete answer to any question, so make sure to keep probing different types of interests\"\"\"\n",
    "    )\n",
    "question_type = QuestionType.OPEN\n",
    "implementation = \"user's interests\"\n",
    "\n",
    "# Variables for the GATE hypothesis_prompt template.\n",
    "prompt1 = \"A user has a particular set of preferences over what articles they would like to read. Based on these preferences, the user has specified whether they are interested in reading following articles.\"\n",
    "prompt2 = \"Based on these preferences, would the user be interested in reading the following article? Only answer \\\"yes\\\" or \\\"no\\\". If uncertain, please make your best guess.\"\n",
    "test_cases = [\n",
    "    (\"Website Name: New York Times\\nTitle: What Does the Future Hold for AI?\", \"Yes\"),\n",
    "    (\"Website Name: CBS Sports\\nTitle: Top Basketball Trends to Watch.\", \"No\"),\n",
    "    (\"Website Name: ESPN\\nTitle: Behind Serena’s Killer Serve.\", \"Yes\"),\n",
    "]\n",
    "\n",
    "# Run GATE\n",
    "previous_examples = gate(num_interactions, task_description, question_type, implementation)\n",
    "\n",
    "# Make a prediction based on the interaction_history\n",
    "gate_test(previous_examples, prompt1, prompt2, test_cases)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
